{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce49da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrallis\n",
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c493a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # wandb params\n",
    "    project: str = \"mocha\"\n",
    "    name: str = \"test_run\"\n",
    "    # training\n",
    "    dataset_path: str = \"data\"\n",
    "    folder_path: str = \"fmtl_small_data\"\n",
    "    ntrials: int = 1\n",
    "    training_percent: float = 0.75\n",
    "    # mocha\n",
    "    w_update: bool = False\n",
    "    inner_iters: int = 100\n",
    "    outer_iters: int = 100\n",
    "    type: str = \"C\"\n",
    "    avg: bool = True\n",
    "    lambd: float = 10\n",
    "    mocha_sdca_frac: float = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def compute_primal(X, Y, W, Omega, lambd):\n",
    "    total_loss = 0\n",
    "    m = len(X)\n",
    "    for t in range(m):\n",
    "        preds = Y[t] * (W[:, t] @ X[t])\n",
    "        total_loss = total_loss + torch.mean(torch.maximum(torch.tensor(0), 1.0 - preds))\n",
    "    primal_obj = total_loss + lambd / 2 * torch.trace(W @ Omega @ W.T)\n",
    "    return primal_obj\n",
    "\n",
    "\n",
    "def compute_dual(alpha, Y, W, Omega, lambd):\n",
    "    total_alpha = 0\n",
    "    m = len(Y)\n",
    "    for tt in range(m):\n",
    "        total_alpha = total_alpha + torch.mean(-1.0 * alpha[tt] * Y[tt])\n",
    "    dual_obj = -lambd / 2 * torch.trace(W @ Omega @ W.T)\n",
    "    return dual_obj\n",
    "\n",
    "\n",
    "# TODO: break down into predict and compute\n",
    "def compute_rmse(X, Y, W, type=\"C\", avg=True):\n",
    "    m = len(X)\n",
    "    Y_hat = []\n",
    "    for t in range(m):\n",
    "        # regression\n",
    "        if type == \"R\":\n",
    "            Y_hat.append(W[:, t] @ X[t])\n",
    "        # classification\n",
    "        else:\n",
    "            Y_hat.append(torch.sign(W[:, t] @ X[t]))\n",
    "\n",
    "    if avg:\n",
    "        all_errs = torch.zeros(m, device=DEVICE)\n",
    "        for t in range(m):\n",
    "            if type == \"R\":\n",
    "                all_errs[t] = torch.sqrt(torch.mean((Y[t] - Y_hat[t]) ** 2))\n",
    "            else:\n",
    "                all_errs[t] = torch.mean(torch.abs(Y[t] - Y_hat[t]) / 2)\n",
    "        err = torch.mean(all_errs)\n",
    "    else:\n",
    "        Y = torch.vstack(Y)\n",
    "        Y_hat = torch.vastack(Y_hat)\n",
    "        if type == \"R\":\n",
    "            err = torch.sqrt(torch.mean((Y - Y_hat) ** 2))\n",
    "        else:\n",
    "            err = torch.mean(torch.abs(Y - Y_hat))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df077",
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.zeros(5) @ torch.zeros(5, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95fe2c",
   "metadata": {},
   "source": [
    "# data loading section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ece025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parce_har(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features = line.strip().split()\n",
    "            data.append(features)\n",
    "    return np.asarray(data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca948ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = \"/Users/agarkov/Downloads/human+activity+recognition+using+smartphones/UCI HAR Dataset/train/\"\n",
    "X_train_path = train_folder_path + \"X_train.txt\"\n",
    "y_train_path = train_folder_path + \"y_train.txt\"\n",
    "\n",
    "test_folder_path = \"/Users/agarkov/Downloads/human+activity+recognition+using+smartphones/UCI HAR Dataset/test/\"\n",
    "X_test_path = test_folder_path + \"X_test.txt\"\n",
    "y_test_path = test_folder_path + \"y_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207916e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = parce_har(X_train_path)\n",
    "Y_train = parce_har(y_train_path)\n",
    "\n",
    "X_test = parce_har(X_test_path)\n",
    "Y_test = parce_har(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c681e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy func to get arrays\n",
    "def dummy_convert(X, y, min_size, max_size, num_datasets):\n",
    "    Xs, ys = [], []\n",
    "    size = y.shape[0]\n",
    "    for t in range(num_datasets):\n",
    "        curr_size = np.random.randint(low=min_size, high=max_size+1)\n",
    "        idxs = np.random.choice(np.arange(size), curr_size, replace=False)\n",
    "        # [num_features, batch_size]\n",
    "        Xs.append(torch.tensor(X[idxs].T, device=DEVICE))\n",
    "        ys.append(torch.tensor(y[idxs], device=DEVICE))\n",
    "        \n",
    "    return Xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d4046",
   "metadata": {},
   "source": [
    "# in case we have mulitiple files representing nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85878a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a026fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_parce_har(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            features = line.strip().split(',')\n",
    "            data.append(features)\n",
    "    return np.asarray(data, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_parser(path: str):\n",
    "    X_files = []\n",
    "    Y_files = []\n",
    "    for subdir, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.startswith(\"X\"):\n",
    "                X_files.append(file)\n",
    "            if file.startswith(\"Y\"):\n",
    "                Y_files.append(file)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    X_files.sort()\n",
    "    Y_files.sort()\n",
    "        \n",
    "    for curr_X, curr_Y in zip(X_files, Y_files):\n",
    "        # in case X_i.txt and Y_i.txt namings\n",
    "        assert curr_X[1:] == curr_Y[1:]\n",
    "        X.append(torch.tensor(comma_parce_har(os.path.join(path, curr_X)).T))\n",
    "        Y.append(torch.tensor(comma_parce_har(os.path.join(path, curr_Y))))\n",
    "        assert X[-1].shape[-1] == Y[-1].shape[0]\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = files_parser(\"fmtl_small_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b571d9e",
   "metadata": {},
   "source": [
    "# running train section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pyrallis.wrap()\n",
    "def train(cfg: TrainConfig):\n",
    "    # Xtrain = [Xtrain_1, Xtrain_2, ..., Xtrain_m] data from m nodes\n",
    "    # Xtrain_i = [num_features, batch_size_i]\n",
    "    Xtrain, Ytrain, Xtest, Ytest = files_parser(cfg.folder_path)\n",
    "    \n",
    "    m = len(Xtrain)\n",
    "    d = Xtrain[0].shape[0]\n",
    "    W = torch.zeros(d, m, device=DEVICE)\n",
    "    alpha = []\n",
    "    Sigma = torch.eye(m, device=DEVICE) / m\n",
    "    Omega = torch.inverse(Sigma)\n",
    "    totaln = 0\n",
    "    n = np.zeros(m, dtype=np.int32)\n",
    "    loss = []\n",
    "    for t in range(m):\n",
    "        n[t] = Ytrain[t].shape[0]\n",
    "        totaln += n[t]\n",
    "        alpha.append(torch.zeros(n[t], device=DEVICE))\n",
    "\n",
    "    rho = 1\n",
    "    if cfg.w_update:\n",
    "        rmse = torch.zeros(cfg.inner_iters)\n",
    "        dual_objs = torch.zeros(cfg.inner_iters)\n",
    "        primal_objs = torch.zeros(cfg.inner_iters)\n",
    "    else:\n",
    "        rmse = torch.zeros(cfg.outer_iters)\n",
    "        dual_objs = torch.zeros(cfg.outer_iters)\n",
    "        primal_objs = torch.zeros(cfg.outer_iters)\n",
    "\n",
    "    for h in trange(cfg.outer_iters):\n",
    "        if not cfg.w_update:\n",
    "            curr_err = compute_rmse(Xtest, Ytest, W, type=\"C\", avg=True)\n",
    "            rmse[h] = curr_err\n",
    "            loss.append(curr_err)\n",
    "            print(curr_err)\n",
    "            primal_objs[h] = compute_primal(Xtrain, Ytrain, W, Omega, cfg.lambd)\n",
    "            dual_objs[h] = compute_dual(alpha, Ytrain, W, Omega, cfg.lambd)\n",
    "\n",
    "        for hh in range(cfg.inner_iters):\n",
    "            # TODO: set rng here\n",
    "            if cfg.w_update:\n",
    "                rmse[hh] = compute_rmse(Xtest, Ytest, W, type=\"C\", avg=True)\n",
    "                primal_objs[hh] = compute_primal(Xtrain, Ytrain, W, Omega, cfg.lambd)\n",
    "                dual_objs[hh] = compute_dual(alpha, Ytrain, W, Omega, cfg.lambd)\n",
    "                \n",
    "            deltaW = torch.zeros((d, m), device=DEVICE)\n",
    "            deltaB = torch.zeros((d, m), device=DEVICE)\n",
    "            for t in range(m):\n",
    "                tperm = torch.randperm(n[t])\n",
    "                alpha_t = alpha[t]\n",
    "                curr_sig = Sigma[t, t]\n",
    "                local_iters = int(n[t] * cfg.mocha_sdca_frac)\n",
    "                \n",
    "                curr_err = compute_rmse(Xtest, Ytest, W, type=\"C\", avg=True)\n",
    "                loss.append(curr_err)\n",
    "                print(curr_err)\n",
    "\n",
    "                for s in range(local_iters):\n",
    "                    # select random coordinate\n",
    "                    idx = tperm[s % n[t]]\n",
    "                    alpha_old = alpha_t[idx].clone()\n",
    "                    curr_y = Ytrain[t][idx]\n",
    "                    curr_x = Xtrain[t][:, idx]\n",
    "\n",
    "                    # compute update\n",
    "                    update = curr_y * curr_x @ (W[:, t] + rho * deltaW[:, t])\n",
    "                    grad = cfg.lambd * n[t] * (1.0 - update) / (\n",
    "                        curr_sig * rho * (curr_x.T @ curr_x)\n",
    "                    ) + (alpha_old * curr_y)\n",
    "                    alpha_t[idx] = curr_y * torch.maximum(torch.tensor(0.0), torch.minimum(torch.tensor(1.0), grad))\n",
    "                    deltaW[:, t] += Sigma[t, t] * (alpha_t[idx] - alpha_old) * curr_x / (cfg.lambd * n[t])\n",
    "                    deltaB[:, t] += (alpha_t[idx] - alpha_old) * curr_x / n[t]\n",
    "                    alpha[t] = alpha_t\n",
    "            for t in range(m):\n",
    "                for tt in range(m):\n",
    "                    W[:, t] += deltaB[:, tt] * Sigma[t, tt] / cfg.lambd\n",
    "\n",
    "        \n",
    "        \n",
    "        # make sure eigenvaluers are positive\n",
    "        A = W.T @ W\n",
    "        if torch.any(torch.linalg.eigvals(A).real < 0):\n",
    "            Dmat, V = torch.linalg.eig(A)\n",
    "            Dmat, V = Dmat.real, V.real\n",
    "            Dmat[Dmat <= 1e-7] = 1e-7\n",
    "            D_c = torch.diag(Dmat)\n",
    "            A = (V @ D_c @ V.T)\n",
    "\n",
    "        sqm = torch.tensor(sqrtm(A).real, device=DEVICE)\n",
    "        Sigma = sqm / torch.trace(sqm)\n",
    "        Omega = torch.linalg.inv(Sigma)\n",
    "        rho = torch.max(torch.sum(torch.abs(Sigma), dim=1) / torch.diag(Sigma))\n",
    "        \n",
    "    return W, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f6455",
   "metadata": {},
   "source": [
    "Lib SVM + гетерогенность данных\n",
    "\n",
    "Разбить по PCA датасет (с одним распределением) и замешать по преимущественным распределением\n",
    "\n",
    "Можно на сгенерированных датасетах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5bbb8",
   "metadata": {},
   "source": [
    "# mushrooms dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0197183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f281c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99133b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mush_nodes = []\n",
    "mush_nodes_y = []\n",
    "num_nodes = 10\n",
    "size = X.shape[0] // num_nodes\n",
    "for i in range(num_nodes):\n",
    "    mush_nodes.append(torch.tensor(X[i*size:(i+1)*size].T, dtype=torch.float32))\n",
    "    mush_nodes_y.append(torch.tensor(y[i*size:(i+1)*size], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_parser(folder_path):\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b7802",
   "metadata": {},
   "source": [
    "# LibSVM a9a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train part\n",
    "# !curl -o a9a.txt https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test part\n",
    "# !curl -o a9a_test.txt https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828eaa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"a9a.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21832ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "mush_nodes = []\n",
    "mush_nodes_y = []\n",
    "num_nodes = 10\n",
    "size = X.shape[0] // num_nodes\n",
    "for i in range(num_nodes):\n",
    "    mush_nodes.append(torch.tensor(X[i*size:(i+1)*size].T, dtype=torch.float32))\n",
    "    mush_nodes_y.append(torch.tensor(y[i*size:(i+1)*size], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d898e",
   "metadata": {},
   "source": [
    "# linreg baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02827300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score as acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d564659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X) > 0\n",
    "acc(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009149ff",
   "metadata": {},
   "source": [
    "# LibSVM w8 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df195ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train part\n",
    "!curl -o w8a.txt https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/w8a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8304d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test part\n",
    "!curl -o w8a_test.txt https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/w8a.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf48a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"w8a.txt\" \n",
    "test_file = \"w8a_test.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d341226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_svmlight_file(train_file)\n",
    "train_x, train_y = train[0].toarray(), train[1]\n",
    "\n",
    "test = load_svmlight_file(test_file)\n",
    "test_x, test_y = test[0].toarray(), test[1]\n",
    "\n",
    "train_test_x = np.vstack((train_x, test_x))\n",
    "train_test_y = np.concatenate((train_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65bf7f",
   "metadata": {},
   "source": [
    "# LibSVM phishing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o phishing.txt https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"phishing.txt\" \n",
    "#файл должен лежать в той же деректории, что и notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_svmlight_file(train_file)\n",
    "train_x, train_y = train[0].toarray(), train[1]\n",
    "\n",
    "train_test_x = train_x\n",
    "train_test_y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3ef9c",
   "metadata": {},
   "source": [
    "# K-means for data heterogenity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b104cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74840aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(train_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = 0.8\n",
    "\n",
    "clust_train_x = []\n",
    "clust_train_y = []\n",
    "\n",
    "clust_test_x = []\n",
    "clust_test_y = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    idxs = kmeans.labels_ == i\n",
    "    \n",
    "    curr_x = train_test_x[idxs]\n",
    "    curr_y = train_test_y[idxs]\n",
    "    \n",
    "    size = curr_y.size\n",
    "    \n",
    "    curr_train_x = curr_x[:int(train_part*size)]\n",
    "    curr_train_y = curr_y[:int(train_part*size)]\n",
    "    \n",
    "    curr_test_x = curr_x[int(train_part*size):]\n",
    "    curr_test_y = curr_y[int(train_part*size):]\n",
    "    \n",
    "    clust_train_x.append(torch.tensor(curr_train_x.T, dtype=torch.float32))\n",
    "    clust_train_y.append(torch.tensor(curr_train_y, dtype=torch.float32))\n",
    "    \n",
    "    clust_test_x.append(torch.tensor(curr_test_x.T, dtype=torch.float32))\n",
    "    clust_test_y.append(torch.tensor(curr_test_y, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_parser(folder_path):\n",
    "    return clust_train_x, clust_train_y, clust_test_x, clust_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083372a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "config = TrainConfig()\n",
    "for lambd in [0.00001]:\n",
    "    config.lambd = lambd\n",
    "    W, rmse = train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
